{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92c7d01",
   "metadata": {},
   "source": [
    "# Part 1: Sequential Conversation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9b726",
   "metadata": {},
   "source": [
    "### Imports & API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03051aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, ChatPromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, SystemMessage\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import ConfigurableFieldSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f71d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# below should not be changed\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# you can change this as preferred\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"chatbot_agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de041aea",
   "metadata": {},
   "source": [
    "### Initialize LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87b50576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For normal accurate responses\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ff964",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f2919d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant called Alex.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230de35",
   "metadata": {},
   "source": [
    "### Chatbot Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "590c95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b99cd",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory\n",
    "Based on number of messages. Where if number of messages is more than k, pop oldest messages and create a new summary by adding information from poped messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90f9ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMemory_custom(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "        # print(f\"Initializing ConversationSummaryBufferMemory_custom with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, \n",
    "        keep only the last 'k' messages and \n",
    "        generate new summary by combining information from dropped messages.\n",
    "        \"\"\"\n",
    "        existing_summary: SystemMessage | None = None\n",
    "        old_messages: list[BaseMessage] | None = None\n",
    "\n",
    "        # check if there is already a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary\")\n",
    "            existing_summary = self.messages.pop(0) # remove old summary from messages\n",
    "\n",
    "        # add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "        # check if there is too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            print(\n",
    "                f\">> Found {len(self.messages)} messages, dropping \"\n",
    "                f\"oldest {len(self.messages) - self.k} messages.\")\n",
    "            \n",
    "            # pull out the oldest messages\n",
    "            num_to_drop = len(self.messages) - self.k\n",
    "            old_messages = self.messages[:num_to_drop] # self.messages[:self.k]\n",
    "\n",
    "            # keep only the most recent messages\n",
    "            self.messages = self.messages[-self.k:]\n",
    "\n",
    "        # if no old_messages, no new info to update the summary\n",
    "        if old_messages is None:\n",
    "            print(\">> No old messages to update summary with\")\n",
    "            return\n",
    "        \n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary,\n",
    "                old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(f\">> New summary: {new_summary.content}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "# initialize memory\n",
    "chat_map = {}\n",
    "\n",
    "# function to get memory for specific session id\n",
    "def get_chat_history(session_id: str, llm: ChatOpenAI, k: int = 4) -> ConversationSummaryBufferMemory_custom:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryBufferMemory_custom(llm=llm, k=k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cead5dd",
   "metadata": {},
   "source": [
    "### RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52d1447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key= \"chat_history\",\n",
    "    history_factory_config= [\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOpenAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=10,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d85f7",
   "metadata": {},
   "source": [
    "### Test LLM Chat with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0783f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n",
      ">> No old messages to update summary with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello, James! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 25, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_950f36939b', 'finish_reason': 'stop', 'logprobs': None}, id='run--d6bfd3b7-fe75-45c1-8457-73840d9a85b2-0', usage_metadata={'input_tokens': 25, 'output_tokens': 16, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_k4\", \"llm\": llm, \"k\": 4}\n",
    ")\n",
    "chat_map[\"id_k4\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "86d4bd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Message 1\n",
      "---\n",
      "\n",
      "get_chat_history called with session_id=id_k4 and k=4\n",
      ">> No old messages to update summary with\n",
      "---\n",
      "Message 2\n",
      "---\n",
      "\n",
      "get_chat_history called with session_id=id_k4 and k=4\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: The conversation began with James introducing himself. The AI responded politely, greeting James and asking how it could assist him today. There was no further interaction beyond this initial exchange.\n",
      "---\n",
      "Message 3\n",
      "---\n",
      "\n",
      "get_chat_history called with session_id=id_k4 and k=4\n",
      ">> Found existing summary\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: The conversation started with James introducing himself, and the AI responded politely, greeting James and asking how it could assist him. The current discussion has shifted to a research topic, with James asking about different types of conversational memory. The AI provided a detailed overview of various types, including short-term, long-term, episodic, semantic, personal, and contextual memory, as well as distinctions like dynamic vs. static and explicit vs. implicit memory. The AI offered to provide more detailed explanations or examples if desired.\n",
      "---\n",
      "Message 4\n",
      "---\n",
      "\n",
      "get_chat_history called with session_id=id_k4 and k=4\n",
      ">> Found existing summary\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: The conversation began with James introducing himself and discussing types of conversational memory, with the AI providing a detailed overview of short-term, long-term, episodic, semantic, personal, and contextual memory, including distinctions like dynamic vs. static and explicit vs. implicit memory. The discussion then shifted to specific AI memory frameworks, with James mentioning interest in ConversationBufferMemory and ConversationBufferWindowMemory. The AI explained that these are specialized memory types used in frameworks like LangChain to manage conversation history. ConversationBufferMemory stores the entire conversation history, suitable for contexts requiring full recall but potentially resource-intensive as it grows. ConversationBufferWindowMemory maintains a sliding window of the most recent exchanges, helping manage token limits and focusing on recent context. The AI provided code examples for both and summarized their features, differences, and use cases, offering guidance on choosing between them based on project needs.\n"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate([\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]):\n",
    "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_k4\", \"llm\": llm, \"k\": 4}\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_Learning_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
