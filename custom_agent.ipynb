{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8081ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import asyncio\n",
    "from typing import Any, Union, Optional\n",
    "from pydantic import SecretStr, BaseModel, Field\n",
    "\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField#, ConfigurableFieldSpec\n",
    "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79c185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03405dfc",
   "metadata": {},
   "source": [
    "### LLM & Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99108761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM and Prompt Setup\n",
    "llm_agent = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    temperature=0.0,\n",
    "    streaming=True,\n",
    ").configurable_fields(\n",
    "    callbacks=ConfigurableField(\n",
    "        id=\"callbacks\",\n",
    "        name=\"callbacks\",\n",
    "        description=\"A list of callbacks to use for streaming\",\n",
    "    )\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a conversational AI assistant that relies solely on the conversation history and tool outputs as your sources of truth.\n",
    "Always use tools to answer the user's current question not previous questions before responding.\n",
    "When you have enough information, use the final_answer tool to provide the final response.\n",
    "\n",
    "Guidelines:\n",
    "1. Use only chat history or tool results — never invent or assume facts.\n",
    "2. If information is missing, ask one short clarifying question.\n",
    "3. If the user changes topic or interrupts, handle it naturally and retain relevant context.\n",
    "4. Be concise, factual, and context-aware.\n",
    "5. When resuming after an interruption, reuse past context only if relevant.\n",
    "6. When exucting a calculation take note of rules of arithmetic order of operations:\n",
    "    a. Parentheses — \" ( \" , \" ) \"\n",
    "    b. Exponentiation — \" ^ \" , \" ** \"\n",
    "    c. Multiplication and division — \" * \" , \" / \"\n",
    "    d. Addition and subtraction — \" + \" , \" - \"\n",
    "\n",
    "Your goal is to respond clearly, naturally, and accurately across turns.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9fc6c",
   "metadata": {},
   "source": [
    "### Chat Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfa06bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMemory_custom(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"\n",
    "    Based on number of messages. Where if number of messages is more than k, \n",
    "    pop oldest messages and create a new summary by adding information from poped messages.\n",
    "    \"\"\"\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: Any = None\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: Any, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "        # print(f\"Initializing ConversationSummaryBufferMemory_custom with k={k}\")\n",
    "\n",
    "    async def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, \n",
    "        keep only the last 'k' messages and \n",
    "        generate new summary by combining information from dropped messages.\n",
    "        \"\"\"\n",
    "        existing_summary: SystemMessage | None = None\n",
    "        old_messages: list[BaseMessage] | None = None\n",
    "\n",
    "        # check if there is already a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            # print(\">> Found existing summary\")\n",
    "            existing_summary = self.messages.pop(0) # remove old summary from messages\n",
    "\n",
    "        # add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "        # check if there is too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            # print(\n",
    "            #     f\">> Found {len(self.messages)} messages, dropping \"\n",
    "            #     f\"oldest {len(self.messages) - self.k} messages.\")\n",
    "            \n",
    "            # pull out the oldest messages\n",
    "            num_to_drop = len(self.messages) - self.k\n",
    "            old_messages = self.messages[:num_to_drop] # self.messages[:self.k]\n",
    "\n",
    "            # keep only the most recent messages\n",
    "            self.messages = self.messages[-self.k:]\n",
    "\n",
    "        # if no old_messages, no new info to update the summary\n",
    "        if old_messages is None:\n",
    "            # print(\">> No old messages to update summary with\")\n",
    "            return\n",
    "        \n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary,\n",
    "                old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # call synchronous llm.invoke in a thread so we don't block the event loop:\n",
    "        # loop = asyncio.get_running_loop()\n",
    "        \n",
    "        # new_summary = await loop.run_in_executor(\n",
    "        #     None,  # default ThreadPoolExecutor\n",
    "        #     lambda: self.llm.invoke(\n",
    "        #         summary_prompt.format_messages(\n",
    "        #             existing_summary=existing_summary,\n",
    "        #             old_messages=old_messages\n",
    "        #         )\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        # print(f\">> New summary: {new_summary.content}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "# function to get memory for specific session id\n",
    "def get_chat_history(session_id: str, llm: ChatOpenAI, k: int = 4) -> ConversationSummaryBufferMemory_custom:\n",
    "    # print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryBufferMemory_custom(llm=llm, k=k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]\n",
    "\n",
    "chat_map = {}\n",
    "llm_memory = ChatOpenAI(temperature=0.0, model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d78738",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8080673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools definition\n",
    "# note: all tools as async to simplify later code\n",
    "@tool\n",
    "async def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "async def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@tool\n",
    "async def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x ** y\n",
    "\n",
    "@tool\n",
    "async def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
    "    return y - x\n",
    "\n",
    "@tool\n",
    "async def divide(x: float, y: float) -> float:\n",
    "    \"\"\"Divide 'x' by 'y'.\"\"\"\n",
    "    if y > 0:\n",
    "        return x / y\n",
    "    else:\n",
    "        return \"Division by error y must be more than 0\"\n",
    "\n",
    "@tool\n",
    "async def final_answer(answer: str, tools_used: list[str]) -> dict[str, str | list[str]]:\n",
    "    \"\"\"Use this tool to provide a final answer to the user.\"\"\"\n",
    "    return {\"answer\": answer, \"tools_used\": tools_used}\n",
    "\n",
    "tools = [add, subtract, multiply, exponentiate, divide, final_answer]\n",
    "# note when we have sync tools we use tool.func, when async we use tool.coroutine\n",
    "name2tool = {tool.name: tool.coroutine for tool in tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2344a5",
   "metadata": {},
   "source": [
    "### Streaming Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "832883ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming Handler\n",
    "class QueueCallbackHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler that puts tokens into a queue.\"\"\"\n",
    "\n",
    "    def __init__(self, queue: asyncio.Queue):\n",
    "        self.queue = queue\n",
    "        self.final_answer_seen = False\n",
    "\n",
    "    async def __aiter__(self): # outputs tokens\n",
    "        while True:\n",
    "            if self.queue.empty():\n",
    "                await asyncio.sleep(0.1)\n",
    "                continue\n",
    "            token_or_done = await self.queue.get()\n",
    "            if token_or_done == \"<<DONE>>\":\n",
    "                # this means we're done\n",
    "                return\n",
    "            if token_or_done:\n",
    "                yield token_or_done\n",
    "    \n",
    "    async def on_llm_new_token(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Put new token in the queue.\"\"\"\n",
    "\n",
    "        #print(f\"on_llm_new_token: {args}, {kwargs}\")\n",
    "        chunk = kwargs.get(\"chunk\")\n",
    "        # check for final_answer tool call\n",
    "        if chunk and chunk.message.additional_kwargs.get(\"tool_calls\"):\n",
    "            if chunk.message.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"] == \"final_answer\":\n",
    "                # this will allow the stream to end on the next `on_llm_end` call\n",
    "                self.final_answer_seen = True\n",
    "                \n",
    "        self.queue.put_nowait(kwargs.get(\"chunk\")) #store tokens into queue\n",
    "    \n",
    "    async def on_llm_end(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Put DONE in the queue to signal completion.\"\"\"\n",
    "\n",
    "        #print(f\"on_llm_end: {args}, {kwargs}\")\n",
    "        # this should only be used at the end of our agent execution, however LangChain\n",
    "        # will call this at the end of every tool call, not just the final tool call\n",
    "        # so we must only send the \"done\" signal if we have already seen the final_answer\n",
    "\n",
    "        if self.final_answer_seen:\n",
    "            self.queue.put_nowait(\"<<DONE>>\")\n",
    "        else:\n",
    "            self.queue.put_nowait(\"<<STEP_END>>\")\n",
    "\n",
    "async def execute_tool(tool_call: AIMessage) -> ToolMessage:\n",
    "    tool_name = tool_call.tool_calls[0][\"name\"]\n",
    "    tool_args = tool_call.tool_calls[0][\"args\"]\n",
    "    tool_out = await name2tool[tool_name](**tool_args)\n",
    "    return ToolMessage(\n",
    "        content=f\"{tool_out}\",\n",
    "        tool_call_id=tool_call.tool_calls[0][\"id\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d10365",
   "metadata": {},
   "source": [
    "### Agent Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e667f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Executor\n",
    "class CustomAgentExecutor:\n",
    "    def __init__(self, max_iterations: int = 5):\n",
    "        self.chat_history: list[BaseMessage] = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt\n",
    "            | llm_agent.bind_tools(tools, tool_choice=\"required\")\n",
    "        )\n",
    "\n",
    "        # In-memory trace list for inspection\n",
    "        self.decision_trace: list[dict] = []\n",
    "\n",
    "    async def invoke(self, input: str, \n",
    "                     streamer: QueueCallbackHandler, \n",
    "                     verbose: bool = False, \n",
    "                     chat_memory: Optional[Union[list[BaseMessage], object]] = None, \n",
    "                     session_id: str = \"session_id_00\") -> dict:\n",
    "        # --- pick the chat history container to use for this invocation ---\n",
    "        # support memory object (with .messages) or plain list\n",
    "        if chat_memory is None:\n",
    "            # fallback (existing behavior)\n",
    "            chat_container = self.chat_history\n",
    "            use_memory_api = False\n",
    "        else:\n",
    "            # detect if memory object (duck-typing)\n",
    "            if hasattr(chat_memory, \"messages\") and hasattr(chat_memory, \"add_messages\"):\n",
    "                chat_container = chat_memory  # memory object\n",
    "                use_memory_api = True\n",
    "\n",
    "            elif isinstance(chat_memory, list):\n",
    "                chat_container = chat_memory\n",
    "                use_memory_api = False\n",
    "            else:\n",
    "                # Unexpected type — fall back to list view if possible\n",
    "                raise TypeError(\"chat_memory must be a list or a memory object with .messages/.add_messages\")\n",
    "                                \n",
    "        # invoke the agent but we do this iteratively in a loop until reach a final answer\n",
    "        count = 0\n",
    "        final_answer: str | None = None\n",
    "        agent_scratchpad: list[AIMessage | ToolMessage] = []\n",
    "        # streaming function\n",
    "        async def stream(query: str) -> list[AIMessage]:\n",
    "            # get the current messages list to pass to the agent prompt\n",
    "            if use_memory_api:\n",
    "                history_for_prompt = chat_container.messages\n",
    "            else:\n",
    "                history_for_prompt = chat_container\n",
    "\n",
    "            configured_agent = self.agent.with_config(\n",
    "                callbacks=[streamer]\n",
    "            )\n",
    "            # Initialize the output dictionary that will be populating with streamed output\n",
    "            outputs = []\n",
    "            # now begin streaming\n",
    "            async for token in configured_agent.astream({\n",
    "                \"input\": query,\n",
    "                \"chat_history\": history_for_prompt,\n",
    "                \"agent_scratchpad\": agent_scratchpad\n",
    "            }):\n",
    "                tool_calls = token.additional_kwargs.get(\"tool_calls\")\n",
    "                if tool_calls: # -> outputs = [tool1, tool2, tool3]\n",
    "                    # first check if have a tool call id - this indicates a new tool\n",
    "                    if tool_calls[0][\"id\"]:\n",
    "                        outputs.append(token)\n",
    "                    else:\n",
    "                        outputs[-1] += token\n",
    "                else:\n",
    "                    pass\n",
    "            return [\n",
    "                AIMessage(\n",
    "                    content=x.content,\n",
    "                    tool_calls=x.tool_calls,\n",
    "                    tool_call_id=x.tool_calls[0][\"id\"]\n",
    "                ) for x in outputs\n",
    "            ]\n",
    "\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            tool_calls = await stream(query=input)\n",
    "            # gather tool execution coroutines\n",
    "            tool_obs = await asyncio.gather(\n",
    "                *[execute_tool(tool_call) for tool_call in tool_calls]\n",
    "            )\n",
    "            # append tool calls and tool observations to the scratchpad in order\n",
    "            id2tool_obs = {tool_call.tool_call_id: tool_obs for tool_call, tool_obs in zip(tool_calls, tool_obs)}\n",
    "            for tool_call in tool_calls:\n",
    "                agent_scratchpad.extend([\n",
    "                    tool_call,\n",
    "                    id2tool_obs[tool_call.tool_call_id]\n",
    "                ])\n",
    "\n",
    "            # ------------------ TOOL-USAGE LOGGING ------------------\n",
    "            # Only log when a tool was requested this iteration\n",
    "            if tool_calls:\n",
    "                # Use first tool_call as representative (agent may call multiple tools)\n",
    "                tc = tool_calls[0]\n",
    "                tool_name = tc.tool_calls[0].get(\"name\")\n",
    "                tool_args = tc.tool_calls[0].get(\"args\", {})\n",
    "                obs = id2tool_obs.get(tc.tool_call_id)\n",
    "                tool_result_summary = getattr(obs, \"content\", str(obs)) if obs is not None else None\n",
    "\n",
    "                trace_entry = {\n",
    "                    \"session_id\": session_id,\n",
    "                    \"turn\": count + 1,\n",
    "                    \"query\": input,\n",
    "                    \"tool\": tool_name,\n",
    "                    \"tool_args\": tool_args,\n",
    "                    \"tool_result_summary\": tool_result_summary,\n",
    "                    \"timestamp\": datetime.now(timezone.utc) #datetime.now(datetime.timezone.utc) + \"Z\"\n",
    "                }\n",
    "\n",
    "                # keep the trace in memory and print it (for screenshots / manual inspection)\n",
    "                self.decision_trace.append(trace_entry)\n",
    "                # pretty print the trace to console (one-line)\n",
    "                # print(\"PLANNER TRACE:\", trace_entry)\n",
    "            \n",
    "            count += 1\n",
    "            # if the tool call is the final answer tool, then stop\n",
    "            found_final_answer = False\n",
    "            for tool_call in tool_calls:\n",
    "                if tool_call.tool_calls[0][\"name\"] == \"final_answer\":\n",
    "                    final_answer_call = tool_call.tool_calls[0]\n",
    "                    final_answer = final_answer_call[\"args\"][\"answer\"]\n",
    "                    found_final_answer = True\n",
    "                    break\n",
    "            \n",
    "            # Only break the loop if found a final answer\n",
    "            if found_final_answer:\n",
    "                break\n",
    "            \n",
    "        # --- write final messages back into the chat memory (or list) ---\n",
    "        human_msg = HumanMessage(content=input)\n",
    "        ai_msg = AIMessage(content=final_answer if final_answer else \"No answer found\")\n",
    "\n",
    "        # If the agent exited due to reaching max iteration limit, ensure the streamer stops cleanly\n",
    "        if not found_final_answer and count >= self.max_iterations:\n",
    "            print(f\"[WARN] Max iteration limit ({self.max_iterations}) reached — stopping agent loop.\")\n",
    "            try:\n",
    "                streamer.queue.put_nowait(\"<<DONE>>\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if use_memory_api:\n",
    "            # use memory object's API (ConversationSummaryBufferMemory_custom.add_messages)\n",
    "            # Note: add_messages expects a list[BaseMessage]\n",
    "            try:\n",
    "                await chat_container.add_messages([human_msg, ai_msg])\n",
    "                \n",
    "            except Exception as e:\n",
    "                # fallback: append to the messages list\n",
    "                chat_container.messages.extend([human_msg, ai_msg])\n",
    "        else:\n",
    "            # plain list\n",
    "            chat_container.extend([human_msg, ai_msg])\n",
    "\n",
    "        # return the final answer in dict form\n",
    "        return final_answer_call if final_answer else {\"answer\": \"No answer found or iteration limit reached\"}\n",
    "\n",
    "# Initialize agent executor\n",
    "agent_executor = CustomAgentExecutor() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3906e5b",
   "metadata": {},
   "source": [
    "### Token Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e315e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming function\n",
    "async def token_generator(\n",
    "        content: str, \n",
    "        streamer: QueueCallbackHandler, \n",
    "        chat_memory: Optional[Union[list[BaseMessage], object]] = None, \n",
    "        session_id: str = \"session_id_00\"):\n",
    "    \n",
    "    task = asyncio.create_task(agent_executor.invoke(\n",
    "        input=content,\n",
    "        streamer=streamer,\n",
    "        verbose=True,  # set to True to see verbose output in console\n",
    "        chat_memory = chat_memory,\n",
    "        session_id = session_id\n",
    "    ))\n",
    "    \n",
    "    # initialize various components to stream\n",
    "    async for token in streamer:\n",
    "        try:\n",
    "            if token == \"<<STEP_END>>\":\n",
    "                # send end of step token\n",
    "                # yield \"</step>\"\n",
    "                print(\"</step>\", flush=True)\n",
    "\n",
    "            elif tool_calls := token.message.additional_kwargs.get(\"tool_calls\"):\n",
    "                if tool_name := tool_calls[0][\"function\"][\"name\"]:\n",
    "                    # send start of step token followed by step name tokens\n",
    "                    # yield f\"<step><step_name>{tool_name}</step_name>\"\n",
    "                    print(f\"<step><step_name>{tool_name}</step_name>\", flush=True)\n",
    "\n",
    "                if tool_args := tool_calls[0][\"function\"][\"arguments\"]:\n",
    "                    # tool args are streamed directly, ensure it's properly encoded\n",
    "                    # yield tool_args\n",
    "                    print(f\"{tool_args}\", end=\"\", flush=True)\n",
    "\n",
    "                # print(f\"\\n{tool_calls[0]}\", end=\"\", flush=True)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error streaming token: {e}\")\n",
    "            continue\n",
    "\n",
    "    final_answer_call = await task\n",
    "    print(\"\\n\")\n",
    "    print(final_answer_call)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    if final_answer_call[\"args\"]:\n",
    "        print(f\"Bot Output: {final_answer_call[\"args\"][\"answer\"]}\")\n",
    "        print(f\"Tools Used: {final_answer_call[\"args\"][\"tools_used\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0867e825",
   "metadata": {},
   "source": [
    "### Print Chat History Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a7fa2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints Conversation History\n",
    "def print_history(session_id: str):\n",
    "    print(\"\\n===== Conversation history =====\")\n",
    "    history = chat_map.get(session_id)\n",
    "\n",
    "    if history is None:\n",
    "        print(\"(no history)\")\n",
    "        return\n",
    "    \n",
    "    for msg in history.messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            role_label = \"Human\"\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            role_label = \"AI\"\n",
    "        elif isinstance(msg, SystemMessage):\n",
    "            role_label = \"System (Summary)\"\n",
    "        else:\n",
    "            role_label = msg.__class__.__name__  # fallback to type name\n",
    "\n",
    "        content = getattr(msg, \"content\", str(msg))\n",
    "        print(f\"\\n{role_label}: {content}\")\n",
    "    \n",
    "    print(\"============================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e980244",
   "metadata": {},
   "source": [
    "### Calculator Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d4d9c",
   "metadata": {},
   "source": [
    "### Successful Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67d538fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<step><step_name>add</step_name>\n",
      "{\"x\": 3, \"y\": 6}<step><step_name>divide</step_name>\n",
      "{\"x\": 5, \"y\": 3}</step>\n",
      "<step><step_name>multiply</step_name>\n",
      "{\"x\": 3, \"y\": 6}<step><step_name>divide</step_name>\n",
      "{\"x\": 5, \"y\": 3}</step>\n",
      "<step><step_name>add</step_name>\n",
      "{\"x\": 3, \"y\": 6}<step><step_name>divide</step_name>\n",
      "{\"x\": 5, \"y\": 3}</step>\n",
      "<step><step_name>multiply</step_name>\n",
      "{\"x\": 2, \"y\": 1.6666666666666667}<step><step_name>exponentiate</step_name>\n",
      "{\"x\": 9, \"y\": 3}</step>\n",
      "<step><step_name>multiply</step_name>\n",
      "{\"x[WARN] Max iteration limit (5) reached — stopping agent loop.\n",
      "\":3.3333333333333335,\"y\":729}</step>\n",
      "\n",
      "\n",
      "{'answer': 'No answer found or iteration limit reached'}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m chat_memory \u001b[38;5;241m=\u001b[39m get_chat_history(session_id\u001b[38;5;241m=\u001b[39msession_id, llm\u001b[38;5;241m=\u001b[39mllm_memory, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m      7\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2*(5*(3+6)/3)^3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m token_generator(content, streamer, chat_memory\u001b[38;5;241m=\u001b[39mchat_memory, session_id\u001b[38;5;241m=\u001b[39msession_id)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# conversation history\u001b[39;00m\n\u001b[0;32m     11\u001b[0m print_history(session_id)\n",
      "Cell \u001b[1;32mIn[11], line 45\u001b[0m, in \u001b[0;36mtoken_generator\u001b[1;34m(content, streamer, chat_memory, session_id)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_answer_call)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot Output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_answer_call[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_answer_call[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools_used\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools Used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_answer_call[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools_used\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'args'"
     ]
    }
   ],
   "source": [
    "queue: asyncio.Queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "session_id = \"test_1\"\n",
    "k = 6\n",
    "chat_memory = get_chat_history(session_id=session_id, llm=llm_memory, k=k)\n",
    "\n",
    "content = \"2*(5*(3+6)/3)^3\"\n",
    "await token_generator(content, streamer, chat_memory=chat_memory, session_id=session_id)\n",
    "\n",
    "# conversation history\n",
    "print_history(session_id)\n",
    "\n",
    "# tool usage logs\n",
    "print(\"Tool Usage Log:\")\n",
    "for log in agent_executor.decision_trace:\n",
    "    print(log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_Learning_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
